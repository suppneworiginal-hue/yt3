"""–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —ñ—Å—Ç–æ—Ä—ñ–π –¥–ª—è YouTube - Streamlit App (Stage 1)"""

import re
import json
import difflib
import streamlit as st
from core.config import STORY_CORE_PROMPT_PATH, STORY_PROMPT_PATH, ensure_cache_dir, COOKIES_FILE, STORY_CORE_PROMPT_FILENAME
from core.utils import safe_int, cookies_file_path
from services.prompts import (
    load_template_from_file,
    load_prompt_file,
    get_default_story_core_template,
    get_default_story_template,
    fill_story_core_prompt,
    fill_story_prompt,
    inject_subtitles_into_prompt,
    inject_story_core_into_prompt,
    inject_all_story_variables
)
from core.config import STORY_CORE_PROMPT_PATH
from services.generators import (
    fetch_and_clean_subtitles,
    generate_story_core
)
from services.llm_backends import generate_text


def initialize_session_state():
    """Initialize all session state variables on first run."""
    if 'youtube_url' not in st.session_state:
        st.session_state.youtube_url = ""
    if 'raw_subtitles' not in st.session_state:
        st.session_state.raw_subtitles = ""
    if 'clean_subtitles' not in st.session_state:
        st.session_state.clean_subtitles = ""
    if 'original_length_chars' not in st.session_state:
        st.session_state.original_length_chars = 0
    if 'story_core_prompt_template' not in st.session_state:
        # Load from file (must exist, no fallback)
        try:
            template = load_prompt_file(str(STORY_CORE_PROMPT_PATH))
            st.session_state.story_core_prompt_template = template
        except FileNotFoundError as e:
            st.error(
    f"–ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –ø—Ä–æ–º–ø—Ç—É –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {STORY_CORE_PROMPT_FILENAME}. –†–æ–∑–º—ñ—Å—Ç—ñ—Ç—å —Ñ–∞–π–ª –≤ –∫–æ—Ä–µ–Ω—ñ –ø—Ä–æ—î–∫—Ç—É.")
            st.session_state.story_core_prompt_template = ""
    if 'story_prompt_template' not in st.session_state:
        # Load from file (must exist, no fallback)
        try:
            template = load_prompt_file(str(STORY_PROMPT_PATH))
            st.session_state.story_prompt_template = template
        except FileNotFoundError as e:
            st.error(
    f"–ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –ø—Ä–æ–º–ø—Ç—É –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: prompt_story.txt. –†–æ–∑–º—ñ—Å—Ç—ñ—Ç—å —Ñ–∞–π–ª –≤ –∫–æ—Ä–µ–Ω—ñ –ø—Ä–æ—î–∫—Ç—É.")
            st.session_state.story_prompt_template = ""
    if 'story_core_text' not in st.session_state:
        st.session_state.story_core_text = ""
    if 'story_core_text_pending' not in st.session_state:
        st.session_state.story_core_text_pending = None
    if 'story_core_prompt_filled' not in st.session_state:
        st.session_state.story_core_prompt_filled = ""
    if 'story_core_prompt_text' not in st.session_state:
        # Initialize with template from file
        try:
            template = load_prompt_file(str(STORY_CORE_PROMPT_PATH))
            st.session_state.story_core_prompt_text = template
            # Also keep the template for reference
            st.session_state.story_core_prompt_template = template
        except FileNotFoundError:
            st.session_state.story_core_prompt_text = ""
            st.session_state.story_core_prompt_template = ""
    if 'subtitles_text' not in st.session_state:
        st.session_state.subtitles_text = ""
    if 'story_prompt_filled' not in st.session_state:
        st.session_state.story_prompt_filled = ""
    if 'generated_story' not in st.session_state:
        st.session_state.generated_story = ""
    if 'story_core_result' not in st.session_state:
        st.session_state.story_core_result = ""
    if 'story_core_result_hash' not in st.session_state:
        st.session_state.story_core_result_hash = None
    if 'story_variables_hash' not in st.session_state:
        st.session_state.story_variables_hash = None
    if 'story_prompt_text' not in st.session_state:
        # Initialize with template from file
        try:
            template = load_prompt_file(str(STORY_PROMPT_PATH))
            st.session_state.story_prompt_text = template
            st.session_state.story_prompt_template = template
        except FileNotFoundError:
            st.session_state.story_prompt_text = ""
            st.session_state.story_prompt_template = ""
    if 'story_result' not in st.session_state:
        st.session_state.story_result = ""
    if 'story_result_pending' not in st.session_state:
        st.session_state.story_result_pending = None
    if 'sub_lang_mode' not in st.session_state:
        st.session_state.sub_lang_mode = "auto"
    if 'prefer_manual' not in st.session_state:
        st.session_state.prefer_manual = True
    # Initialize debug variables
    if 'debug_last_prompt' not in st.session_state:
        st.session_state.debug_last_prompt = ""
    if 'debug_last_response' not in st.session_state:
        st.session_state.debug_last_response = ""
    if 'debug_prompt_chars' not in st.session_state:
        st.session_state.debug_prompt_chars = 0
    if 'debug_story_core_chars' not in st.session_state:
        st.session_state.debug_story_core_chars = 0
    if 'debug_target_length_chars' not in st.session_state:
        st.session_state.debug_target_length_chars = 0
    if 'debug_response_chars' not in st.session_state:
        st.session_state.debug_response_chars = 0
    if 'debug_error' not in st.session_state:
        st.session_state.debug_error = None
    # Initialize analytics variables
    if 'analysis_story_input' not in st.session_state:
        st.session_state.analysis_story_input = ""
    if 'analysis_story_input_pending' not in st.session_state:
        st.session_state.analysis_story_input_pending = None
    if 'analysis_report' not in st.session_state:
        st.session_state.analysis_report = ""
    if 'comparison_table_md' not in st.session_state:
        st.session_state.comparison_table_md = ""
    if 'improvement_prompt' not in st.session_state:
        st.session_state.improvement_prompt = ""
    if 'improved_story' not in st.session_state:
        st.session_state.improved_story = ""
    # Initialize status tracking variables
    if 'last_status' not in st.session_state:
        st.session_state.last_status = "–ì–æ—Ç–æ–≤–æ –¥–æ —Ä–æ–±–æ—Ç–∏"
    if 'last_status_level' not in st.session_state:
        st.session_state.last_status_level = "info"
    if 'last_action' not in st.session_state:
        st.session_state.last_action = ""
    if 'last_completed_step' not in st.session_state:
        st.session_state.last_completed_step = ""
    if 'last_run_at' not in st.session_state:
        st.session_state.last_run_at = ""
    # Initialize LLM backend and pipeline mode
    if 'llm_backend' not in st.session_state:
        from core.config import LLM_BACKEND_DEFAULT
        st.session_state.llm_backend = LLM_BACKEND_DEFAULT
    if 'pipeline_mode' not in st.session_state:
        st.session_state.pipeline_mode = "classic"
    # Initialize GenAI App configuration (UI-first, env fallback)
    if 'genai_app_url' not in st.session_state:
        from core.config import GENAI_APP_URL
        st.session_state.genai_app_url = GENAI_APP_URL
    if 'genai_app_token' not in st.session_state:
        from core.config import GENAI_APP_TOKEN
        st.session_state.genai_app_token = GENAI_APP_TOKEN


def update_original_length():
    """Recalculate original_length_chars from clean_subtitles."""
    st.session_state.original_length_chars = len(
        st.session_state.clean_subtitles)


def format_slide(text: str, prompt: str) -> str:
    """
    Format a single slide with enforced braces.

    Args:
        text: Slide narration text
        prompt: Voice delivery prompt

    Returns:
        Formatted slide string
    """
    # Trim
    text = text.strip()
    prompt = prompt.strip()

    # Ensure braces
    if not text.startswith("{"):
        text = "{" + text
    if not text.endswith("}"):
        text = text + "}"

    if not prompt.startswith("{"):
        prompt = "{" + prompt
    if not prompt.endswith("}"):
        prompt = prompt + "}"

    return f"Text:\n{text}\n\nPrompt:\n{prompt}"


def show_friendly_error(error: Exception):
    """Display user-friendly error message based on error type."""
    error_msg = str(error)

    # Check for GenAI App configuration issues
    if "–Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ" in error_msg or "not configured" in error_msg.lower(
    ) or "not set" in error_msg.lower():
        st.warning(
            "‚ö†Ô∏è –í–∫–∞–∂–∏ GenAI App URL —É –±–æ–∫–æ–≤—ñ–π –ø–∞–Ω–µ–ª—ñ (Sidebar ‚Üí GenAI App Settings).")
    else:
        # Show generic error
        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {error_msg}")


def update_status(message: str, level: str = "info", action: str = "", step: str = ""):
    """Update status tracking variables."""
    from datetime import datetime
    st.session_state.last_status = message
    st.session_state.last_status_level = level
    st.session_state.last_action = action
    st.session_state.last_completed_step = step
    st.session_state.last_run_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def show_status_display():
    """Display current status area."""
    status_level = st.session_state.get("last_status_level", "info")
    status_message = st.session_state.get("last_status", "–ì–æ—Ç–æ–≤–æ –¥–æ —Ä–æ–±–æ—Ç–∏")
    last_action = st.session_state.get("last_action", "")
    last_step = st.session_state.get("last_completed_step", "")
    last_run = st.session_state.get("last_run_at", "")

    status_text = status_message
    if last_step:
        status_text += f" | –û—Å—Ç–∞–Ω–Ω—ñ–π –∫—Ä–æ–∫: {last_step}"
    if last_run:
        status_text += f" | {last_run}"

    if status_level == "success":
        st.success(status_text)
    elif status_level == "warning":
        st.warning(status_text)
    elif status_level == "error":
        st.error(status_text)
    else:
        st.info(status_text)


def show_readiness_indicators():
    """Show compact readiness indicators."""
    subtitles_ready = bool(st.session_state.get("clean_subtitles"))
    core_ready = bool(st.session_state.get("story_core_text"))
    story_ready = bool(st.session_state.get("generated_story")
                       or st.session_state.get("story_result"))

    subtitles_indicator = "‚úÖ" if subtitles_ready else "‚Äî"
    core_indicator = "‚úÖ" if core_ready else "‚Äî"
    story_indicator = "‚úÖ" if story_ready else "‚Äî"

    st.caption(
    f"–°—Ç–∞—Ç—É—Å: –°—É–±—Ç–∏—Ç—Ä–∏ {subtitles_indicator} | STORY_CORE {core_indicator} | –Ü—Å—Ç–æ—Ä—ñ—è {story_indicator}")


# Analysis prompt template (Ukrainian output)
ANALYSIS_PROMPT_TEMPLATE = """–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –∞–Ω–∞–ª—ñ–∑—É –Ω–∞—Ä–∞—Ç–∏–≤–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤ –¥–ª—è YouTube.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ø—Ä–æ–≤–µ—Å—Ç–∏ —á–µ—Å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ—ó —ñ—Å—Ç–æ—Ä—ñ—ó –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª–æ–º.

–í–•–Ü–î–ù–Ü –î–ê–ù–Ü:

–û–†–ò–ì–Ü–ù–ê–õ (—Å—É–±—Ç–∏—Ç—Ä–∏):
    {ORIGINAL}

–ó–ì–ï–ù–ï–†–û–í–ê–ù–ê –Ü–°–¢–û–†–Ü–Ø:
    {GENERATED}

–ó–ê–í–î–ê–ù–ù–Ø:

1. –û—Ü—ñ–Ω–∏ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—É —ñ—Å—Ç–æ—Ä—ñ—é –∑–∞ —à–∫–∞–ª–æ—é 0-10 –¥–ª—è –∫–æ–∂–Ω–æ—ó –º–µ—Ç—Ä–∏–∫–∏:
    - Hook (—Å–∏–ª–∞ –∑–∞—á—ñ–ø–∫–∏ –≤ –ø–µ—Ä—à–∏—Ö —Å–ª–∞–π–¥–∞—Ö)
   - Retention chain (–Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω—ñ —Ü–∏–∫–ª–∏ / –Ω–∞–ø—Ä—É–≥–∞)
   - Clarity (—è—Å–Ω—ñ—Å—Ç—å –≤–∏–∫–ª–∞–¥—É)
   - Pacing (—Ä–∏—Ç–º, —Ç–µ–º–ø)
   - Repetition (–≤—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–µ–Ω—å)
   - Ending impact (–≤–ø–ª–∏–≤ —Ñ—ñ–Ω–∞–ª—É)

2. –ü–µ—Ä–µ–ª—ñ—á–∏ 3 —Å–∏–ª—å–Ω—ñ —Å—Ç–æ—Ä–æ–Ω–∏ (bullet list)

3. –ü–µ—Ä–µ–ª—ñ—á–∏ 3 —Å–ª–∞–±–∫—ñ —Å—Ç–æ—Ä–æ–Ω–∏ (bullet list)

4. –°—Ç–≤–æ—Ä–∏ —Ç–∞–±–ª–∏—Ü—é –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤ markdown —Ñ–æ—Ä–º–∞—Ç—ñ:
    | –ö—Ä–∏—Ç–µ—Ä—ñ–π | –û—Ä–∏–≥—ñ–Ω–∞–ª | –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ | –ö–æ–º–µ–Ω—Ç–∞—Ä |
   |----------|----------|-------------|----------|
   | Hook | ... | ... | ... |
   | Stakes clarity | ... | ... | ... |
   | Loops | ... | ... | ... |
   | Escalation | ... | ... | ... |
   | Specificity | ... | ... | ... |
   | Ending | ... | ... | ... |

5. –°—Ç–≤–æ—Ä–∏ "–ü—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è" - –≥–æ—Ç–æ–≤–∏–π –ø—Ä–æ–º–ø—Ç, —è–∫–∏–π —ñ–Ω—Å—Ç—Ä—É–∫—Ç—É—î –º–æ–¥–µ–ª—å —è–∫ –ø–µ—Ä–µ–ø–∏—Å–∞—Ç–∏ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—É —ñ—Å—Ç–æ—Ä—ñ—é:
    - –ó–±–µ—Ä–µ–≥—Ç–∏ –∫–ª—é—á–æ–≤—ñ —Ñ–∞–∫—Ç–∏
   - –í–∏–ø—Ä–∞–≤–∏—Ç–∏ –∑–Ω–∞–π–¥–µ–Ω—ñ —Å–ª–∞–±–∫–æ—Å—Ç—ñ
   - –î–æ—Ç—Ä–∏–º—É–≤–∞—Ç–∏—Å—è –ø—Ä–∞–≤–∏–ª —Å—Ç–∏–ª—é: show-don't-tell, —Ä–æ–∑–º–æ–≤–Ω–∏–π —Å—Ç–∏–ª—å, –±–µ–∑ –º–æ—Ä–∞–ª—ñ–∑–∞—Ü—ñ—ó
   - –£–Ω–∏–∫–Ω—É—Ç–∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω—å

   –í–ê–ñ–õ–ò–í–û: –ü—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º–∞—î –±—É—Ç–∏ –ê–ù–ì–õ–Ü–ô–°–¨–ö–û–Æ –ú–û–í–û–Æ –¢–Ü–õ–¨–ö–ò. –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —É–∫—Ä–∞—ó–Ω—Å—å–∫—É —á–∏ —Ä–æ—Å—ñ–π—Å—å–∫—É.

–§–û–†–ú–ê–¢ –í–ò–í–û–î–£ (—Å—Ç—Ä–æ–≥–æ –¥–æ—Ç—Ä–∏–º—É–π—Å—è):

## –û–¶–Ü–ù–ö–ò (0-10)
- Hook: [—á–∏—Å–ª–æ]/10
- Retention chain: [—á–∏—Å–ª–æ]/10
- Clarity: [—á–∏—Å–ª–æ]/10
- Pacing: [—á–∏—Å–ª–æ]/10
- Repetition: [—á–∏—Å–ª–æ]/10
- Ending impact: [—á–∏—Å–ª–æ]/10

## –°–ò–õ–¨–ù–Ü –°–¢–û–†–û–ù–ò
- [–ø–µ—Ä—à–∞]
- [–¥—Ä—É–≥–∞]
- [—Ç—Ä–µ—Ç—è]

## –°–õ–ê–ë–ö–Ü –°–¢–û–†–û–ù–ò
- [–ø–µ—Ä—à–∞]
- [–¥—Ä—É–≥–∞]
- [—Ç—Ä–µ—Ç—è]

## –¢–ê–ë–õ–ò–¶–Ø –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø
[markdown —Ç–∞–±–ª–∏—Ü—è —Ç—É—Ç]

## –ü–†–û–ú–ü–¢ –î–õ–Ø –ü–û–ö–†–ê–©–ï–ù–ù–Ø
[—Ç–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç—É –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è]"""


def parse_analysis_response(response_text: str) -> tuple[str, str, str]:
    """
    Parse LLM analysis response into components.

    Returns:
        Tuple of (analysis_report, comparison_table_md, improvement_prompt)
    """
    analysis_report = ""
    comparison_table_md = ""
    improvement_prompt = ""

    # Find section markers
    table_start = response_text.find("## –¢–ê–ë–õ–ò–¶–Ø –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø")
    prompt_start = response_text.find("## –ü–†–û–ú–ü–¢ –î–õ–Ø –ü–û–ö–†–ê–©–ï–ù–ù–Ø")

    # Extract comparison table (between ## –¢–ê–ë–õ–ò–¶–Ø –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø and ## –ü–†–û–ú–ü–¢
    # –î–õ–Ø –ü–û–ö–†–ê–©–ï–ù–ù–Ø or end)
    if table_start != -1:
        table_end = prompt_start if prompt_start != -1 else len(response_text)
        comparison_table_md = response_text[table_start:table_end].strip()
        # Remove the header
        if comparison_table_md.startswith("## –¢–ê–ë–õ–ò–¶–Ø –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø"):
            comparison_table_md = comparison_table_md[len(
                "## –¢–ê–ë–õ–ò–¶–Ø –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø"):].strip()

    # Extract improvement prompt (after ## –ü–†–û–ú–ü–¢ –î–õ–Ø –ü–û–ö–†–ê–©–ï–ù–ù–Ø)
    if prompt_start != -1:
        improvement_prompt = response_text[prompt_start +
     len("## –ü–†–û–ú–ü–¢ –î–õ–Ø –ü–û–ö–†–ê–©–ï–ù–ù–Ø"):].strip()

    # Analysis report is everything before the table section (or before prompt
    # if no table)
    if table_start != -1:
        analysis_report = response_text[:table_start].strip()
    elif prompt_start != -1:
        analysis_report = response_text[:prompt_start].strip()
    else:
        # If no sections found, return full text as analysis report
        analysis_report = response_text.strip()

    return analysis_report, comparison_table_md, improvement_prompt


def main():
    """Main Streamlit app."""
    st.set_page_config(
        page_title="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —ñ—Å—Ç–æ—Ä—ñ–π –¥–ª—è YouTube",
        page_icon="üì∫",
        layout="wide"
    )

    st.title("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —ñ—Å—Ç–æ—Ä—ñ–π –¥–ª—è YouTube")

    # Ensure cache directory exists
    ensure_cache_dir()

    # Initialize session state
    initialize_session_state()

    # Sidebar for input
    with st.sidebar:
        st.header("–í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ")
        st.session_state.youtube_url = st.text_input(
            "–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ YouTube",
            value=st.session_state.youtube_url,
            help="–í—Å—Ç–∞–≤—Ç–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ YouTube –≤—ñ–¥–µ–æ"
        )

        use_cache = st.checkbox(
            "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∫–µ—à, —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π",
            value=True,
            help="–Ø–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∫–µ—à–æ–≤–∞–Ω—ñ —Å—É–±—Ç–∏—Ç—Ä–∏ –∑–∞–º—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è"
        )

        st.session_state.sub_lang_mode = st.selectbox(
            "–ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –º–æ–≤–∏ —Å—É–±—Ç–∏—Ç—Ä—ñ–≤",
            options=["auto", "en", "uk", "ru"],
            format_func=lambda x: {
                "auto": "–ê–≤—Ç–æ (en ‚Üí uk ‚Üí ru)",
                "en": "–¢—ñ–ª—å–∫–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–∞ (en)",
                "uk": "–¢—ñ–ª—å–∫–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (uk)",
                "ru": "–¢—ñ–ª—å–∫–∏ —Ä–æ—Å—ñ–π—Å—å–∫–∞ (ru)"
            }[x],
            index=[
    "auto", "en", "uk", "ru"].index(
        st.session_state.sub_lang_mode) if st.session_state.sub_lang_mode in [
            "auto", "en", "uk", "ru"] else 0,
            help="–í–∏–±–µ—Ä—ñ—Ç—å –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –º–æ–≤–∏ –¥–ª—è —Å—É–±—Ç–∏—Ç—Ä—ñ–≤"
        )

        st.session_state.prefer_manual = st.checkbox(
            "–ü—Ä–æ–±—É–≤–∞—Ç–∏ —Ä—É—á–Ω—ñ —Å—É–±—Ç–∏—Ç—Ä–∏ –ø–µ—Ä—à–∏–º–∏",
            value=st.session_state.prefer_manual,
            help="–Ø–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–æ, —Å–ø–æ—á–∞—Ç–∫—É —à—É–∫–∞—î —Ä—É—á–Ω—ñ —Å—É–±—Ç–∏—Ç—Ä–∏, –ø–æ—Ç—ñ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω—ñ"
        )

        # Show cookies file status
        cookies_path = cookies_file_path()
        if cookies_path:
            st.caption(
    f"Cookies —Ñ–∞–π–ª –∑–Ω–∞–π–¥–µ–Ω–æ: {COOKIES_FILE} (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó)")
        else:
            st.caption(f"Cookies —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ({COOKIES_FILE}).")

        st.divider()

        # LLM Backend and Pipeline Mode switches
        st.subheader("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è LLM")

        backend_options = ["ChatGPT (OpenAI)", "GenAI App"]
        backend_index = 0 if st.session_state.llm_backend == "openai" else 1
        backend_choice = st.radio(
            "LLM Backend",
            backend_options,
            index=backend_index,
            help="–í–∏–±–µ—Ä—ñ—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—ñ–≤"
        )
        st.session_state.llm_backend = "openai" if backend_choice == "ChatGPT (OpenAI)" else "genai_app"

        # GenAI App configuration inputs (show only when genai_app is selected)
        if st.session_state.llm_backend == "genai_app":
            st.subheader("GenAI App Settings")

            genai_url = st.text_input(
                "GenAI App URL",
                value=st.session_state.get("genai_app_url", ""),
                placeholder="https://your-endpoint.com/run",
                help="–í–∫–∞–∂–∏ –ø–æ–≤–Ω–∏–π URL –µ–Ω–¥–ø–æ—ñ–Ω—Ç—É GenAI App Builder"
            )
            st.session_state.genai_app_url = genai_url.strip()

            genai_token = st.text_input(
                "GenAI App Token (optional)",
                value=st.session_state.get("genai_app_token", ""),
                type="password",
                help="–í–∫–∞–∂–∏ —Ç–æ–∫–µ–Ω –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω)"
            )
            st.session_state.genai_app_token = genai_token.strip()

            st.caption(
                "üí° Tip: –í–∫–ª–µ–π —Å–≤—ñ–π GenAI App Builder endpoint —É –ø–æ–ª–µ –≤–∏—â–µ.")

            # Test connection button
            if st.button("Test GenAI App connection"):
                if not st.session_state.genai_app_url:
                    st.sidebar.warning("‚ö†Ô∏è –°–ø–æ—á–∞—Ç–∫—É –≤–∫–∞–∂–∏ GenAI App URL –≤–∏—â–µ.")
                else:
                    try:
                        import time
                        start = time.time()
                        test_response = generate_text(
    "Reply ONLY with: OK", backend="genai_app")
                        latency = time.time() - start
                        st.sidebar.success(
                            f"‚úÖ –ó'—î–¥–Ω–∞–Ω–æ! –í—ñ–¥–ø–æ–≤—ñ–¥—å: {test_response[:50]} | –ó–∞—Ç—Ä–∏–º–∫–∞: {latency:.2f}s")
                    except Exception as e:
                        error_msg = str(e)
                        if "not configured" in error_msg.lower() or "not set" in error_msg.lower():
                            st.sidebar.error(
                                "‚ùå URL –Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ. –ü–µ—Ä–µ–≤—ñ—Ä –ø–æ–ª–µ –≤–∏—â–µ.")
                        else:
                            st.sidebar.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {error_msg[:200]}")

            st.divider()

        pipeline_options = ["Classic", "Multi-PASS (AI Controlled)"]
        pipeline_index = 0 if st.session_state.pipeline_mode == "classic" else 1
        pipeline_choice = st.radio(
            "Pipeline Mode",
            pipeline_options,
            index=pipeline_index,
            help="Classic: —ñ—Å–Ω—É—é—á—ñ –ø—Ä–æ–º–ø—Ç–∏. Multi-PASS: AI –∫–æ–Ω—Ç—Ä–æ–ª—é—î –≤—Å—ñ –µ—Ç–∞–ø–∏."
        )
        st.session_state.pipeline_mode = "classic" if pipeline_choice == "Classic" else "multipass"

        # Test backend button
        if st.button("–¢–µ—Å—Ç –≤–∏–±—Ä–∞–Ω–æ–≥–æ backend"):
            try:
                from services.llm_backends import generate_text as backend_generate
                import time
                start = time.time()
                test_response = backend_generate(
    "Reply with: OK", backend=st.session_state.llm_backend)
                latency = time.time() - start
                st.success(
                    f"‚úÖ Backend –ø—Ä–∞—Ü—é—î! –í—ñ–¥–ø–æ–≤—ñ–¥—å: {test_response[:100]} | –ó–∞—Ç—Ä–∏–º–∫–∞: {latency:.2f}s")
            except Exception as e:
                st.error(f"‚ùå Backend –ø–æ–º–∏–ª–∫–∞: {str(e)}")

        st.divider()

        if st.button("–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏", type="primary"):
            if not st.session_state.youtube_url or not st.session_state.youtube_url.strip():
                update_status(
    "–ü–æ–º–∏–ª–∫–∞: –Ω–µ –≤–∫–∞–∑–∞–Ω–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ YouTube",
    "error",
    "–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏ (sidebar)",
     "")
                st.error("–ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ YouTube.")
            else:
                try:
                    try:
                        with st.status("–û—Ç—Ä–∏–º—É—é —Å—É–±—Ç–∏—Ç—Ä–∏...", expanded=True) as status:
                            status.write("–ö—Ä–æ–∫ 1/1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—É–±—Ç–∏—Ç—Ä—ñ–≤...")
                        raw_vtt, clean_text, meta = fetch_and_clean_subtitles(
                            st.session_state.youtube_url,
                            lang_mode=st.session_state.sub_lang_mode,
                            prefer_manual=st.session_state.prefer_manual,
                            use_cache=use_cache
                        )
                        st.session_state.raw_subtitles = raw_vtt
                        st.session_state.clean_subtitles = clean_text
                        st.session_state.subtitles_text = clean_text
                        st.session_state.original_length_chars = len(
                            clean_text)

                        # Inject subtitles into the top prompt textarea
                        if st.session_state.story_core_prompt_text:
                            st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                st.session_state.story_core_prompt_text,
                                clean_text
                            )
                        else:
                            # If prompt text is empty, load template and inject
                            try:
                                template = load_prompt_file(
                                    str(STORY_CORE_PROMPT_PATH))
                                st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                    template,
                                    clean_text
                                )
                            except FileNotFoundError:
                                pass

                        source_map = {
                            "cache": "–∫–µ—à",
                            "manual": "—Ä—É—á–Ω—ñ",
                            "auto": "–∞–≤—Ç–æ"
                        }
                        source_text = source_map.get(
                            meta["source"], meta["source"])
                        status.update(
    label="–°—É–±—Ç–∏—Ç—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ ‚úÖ", state="complete")
                        update_status(
    f"–°—É–±—Ç–∏—Ç—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ! –î–∂–µ—Ä–µ–ª–æ: {source_text}",
    "success",
    "–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏ (sidebar)",
     "–°—É–±—Ç–∏—Ç—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
                        try:
                            st.toast("–ì–æ—Ç–æ–≤–æ: –°—É–±—Ç–∏—Ç—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ!", icon="‚úÖ")
                        except:
                            pass
                        st.rerun()
                    except:
                        with st.spinner("–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—É–±—Ç–∏—Ç—Ä—ñ–≤..."):
                            raw_vtt, clean_text, meta = fetch_and_clean_subtitles(
                                st.session_state.youtube_url,
                                lang_mode=st.session_state.sub_lang_mode,
                                prefer_manual=st.session_state.prefer_manual,
                                use_cache=use_cache
                            )
                            st.session_state.raw_subtitles = raw_vtt
                            st.session_state.clean_subtitles = clean_text
                            st.session_state.subtitles_text = clean_text
                            st.session_state.original_length_chars = len(
                                clean_text)

                            if st.session_state.story_core_prompt_text:
                                st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                    st.session_state.story_core_prompt_text,
                                    clean_text
                                )
                            else:
                                try:
                                    template = load_prompt_file(
                                        str(STORY_CORE_PROMPT_PATH))
                                    st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                        template,
                                        clean_text
                                    )
                                except FileNotFoundError:
                                    pass

                            source_map = {
                                "cache": "–∫–µ—à",
                                "manual": "—Ä—É—á–Ω—ñ",
                                "auto": "–∞–≤—Ç–æ"
                            }
                            source_text = source_map.get(
                                meta["source"], meta["source"])
                            update_status(
    f"–°—É–±—Ç–∏—Ç—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ! –î–∂–µ—Ä–µ–ª–æ: {source_text}",
    "success",
    "–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏ (sidebar)",
     "–°—É–±—Ç–∏—Ç—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
                            try:
                                st.toast(
    "–ì–æ—Ç–æ–≤–æ: –°—É–±—Ç–∏—Ç—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ!", icon="‚úÖ")
                            except:
                                pass
                        st.rerun()
                except ValueError as e:
                    update_status(
    f"–ü–æ–º–∏–ª–∫–∞: {
        str(e)}",
        "error",
        "–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏ (sidebar)",
         "")
                    st.error(str(e))
                except Exception as e:
                    update_status(
    f"–ü–æ–º–∏–ª–∫–∞: {
        str(e)}",
        "error",
        "–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏ (sidebar)",
         "")
                    st.error(str(e))

        st.divider()

    # Create top-level tabs
    tab_generate, tab_analytics = st.tabs([
        "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è",
        "–ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ —Ç–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è"
    ])

    # ========================
    # TAB 1: GENERATION
    # ========================
    with tab_generate:
        # ========================
        # ACTION BAR (PRO TOOL)
        # ========================
        st.subheader("–ü–∞–Ω–µ–ª—å –∫–µ—Ä—É–≤–∞–Ω–Ω—è")

        # Readiness indicators
        show_readiness_indicators()

        # Action buttons in columns
        col1, col2, col3, col4, col5 = st.columns(5)

        with col1:
            run_all_button = st.button(
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—Å–µ",
    type="primary",
     use_container_width=True)

        with col2:
            fetch_subtitles_button = st.button(
    "–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏", use_container_width=True)

        with col3:
            generate_core_button = st.button(
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE", use_container_width=True)

        with col4:
            generate_story_button = st.button(
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é", use_container_width=True)

        with col5:
            clear_status_button = st.button(
    "–û—á–∏—Å—Ç–∏—Ç–∏ —Å—Ç–∞—Ç—É—Å", use_container_width=True)

        # Status display area
        st.divider()
        show_status_display()
        st.divider()

        # Handle clear status button
        if clear_status_button:
            update_status("–°—Ç–∞—Ç—É—Å –æ—á–∏—â–µ–Ω–æ", "info", "–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—É", "")
            st.rerun()

        # Handle run all pipeline
        if run_all_button:
            # Check prerequisites
            if not st.session_state.youtube_url or not st.session_state.youtube_url.strip():
                update_status(
    "–ü–æ–º–∏–ª–∫–∞: –Ω–µ –≤–∫–∞–∑–∞–Ω–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ YouTube",
    "error",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—Å–µ",
     "")
                st.error(
                    "–ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ YouTube –≤ –±—ñ—á–Ω—ñ–π –ø–∞–Ω–µ–ª—ñ.")
            else:
                # Try to use st.status, fallback to st.spinner
                try:
                    with st.status("–ó–∞–ø—É—Å–∫ –ø–æ–≤–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω—É...", expanded=True) as status:
                        status.write("–ö—Ä–æ–∫ 1/3: –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—É–±—Ç–∏—Ç—Ä—ñ–≤...")
                        try:
                            use_cache = True  # Use cache by default
                            raw_vtt, clean_text, meta = fetch_and_clean_subtitles(
                                st.session_state.youtube_url,
                                lang_mode=st.session_state.sub_lang_mode,
                                prefer_manual=st.session_state.prefer_manual,
                                use_cache=use_cache
                            )
                            st.session_state.raw_subtitles = raw_vtt
                            st.session_state.clean_subtitles = clean_text
                            st.session_state.subtitles_text = clean_text
                            st.session_state.original_length_chars = len(
                                clean_text)

                            # Inject subtitles into prompt
                            if st.session_state.story_core_prompt_text:
                                st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                    st.session_state.story_core_prompt_text,
                                    clean_text
                                )
                            else:
                                try:
                                    template = load_prompt_file(
                                        str(STORY_CORE_PROMPT_PATH))
                                    st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                        template,
                                        clean_text
                                    )
                                except FileNotFoundError:
                                    pass

                            status.update(
    label="–ö—Ä–æ–∫ 1/3: –°—É–±—Ç–∏—Ç—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ ‚úÖ", state="complete")
                            status.write("–ö—Ä–æ–∫ 2/3: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è STORY_CORE...")

                            # Step 2: Generate STORY_CORE
                            if not st.session_state.story_core_prompt_text or not st.session_state.story_core_prompt_text.strip():
                                raise ValueError("–ü—Ä–æ–º–ø—Ç STORY_CORE –ø–æ—Ä–æ–∂–Ω—ñ–π")

                            prompt_to_send = st.session_state.story_core_prompt_text
                            story_core_output = generate_text(
    prompt_to_send, backend=st.session_state.llm_backend)

                            st.session_state.story_core_text_pending = story_core_output
                            st.session_state.story_core_result = story_core_output
                            import hashlib
                            st.session_state.story_core_result_hash = hashlib.md5(
                                story_core_output.encode()).hexdigest()
                            st.session_state.story_core_prompt_filled = prompt_to_send

                            # Apply pending immediately
                            st.session_state.story_core_text = story_core_output
                            st.session_state.story_core_text_pending = None

                            status.update(
    label="–ö—Ä–æ–∫ 2/3: STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ ‚úÖ",
     state="complete")
                            status.write("–ö—Ä–æ–∫ 3/3: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —ñ—Å—Ç–æ—Ä—ñ—ó...")

                            # Step 3: Generate STORY
                            # Auto-inject variables if needed
                            story_core_str = st.session_state.story_core_result if st.session_state.story_core_result else ""
                            target_length_str = str(
    st.session_state.original_length_chars)
                            combined_vars = f"{story_core_str}|{target_length_str}"
                            current_hash = hashlib.md5(
                                combined_vars.encode()).hexdigest()

                            if (st.session_state.story_variables_hash is None or
                                st.session_state.story_variables_hash != current_hash or
                                not st.session_state.story_prompt_text or
                                st.session_state.story_prompt_text == st.session_state.story_prompt_template):

                                if not st.session_state.story_prompt_text or st.session_state.story_prompt_text == st.session_state.story_prompt_template:
                                    try:
                                        template = load_prompt_file(
                                            str(STORY_PROMPT_PATH))
                                        st.session_state.story_prompt_text = template
                                        st.session_state.story_prompt_template = template
                                    except FileNotFoundError:
                                        pass

                                if st.session_state.story_prompt_text:
                                    st.session_state.story_prompt_text = inject_all_story_variables(
                                        st.session_state.story_prompt_text,
                                        st.session_state.story_core_result if st.session_state.story_core_result else "",
                                        st.session_state.original_length_chars
                                    )
                                    st.session_state.story_variables_hash = current_hash

                            prompt_template = st.session_state.story_prompt_text
                            try:
                                prompt_to_send = fill_story_prompt(
                                    prompt_template,
                                    st.session_state.story_core_result,
                                    st.session_state.original_length_chars,
                                    None
                                )
                            except Exception:
                                prompt_to_send = prompt_template
                                if "{TARGET_LENGTH_CHARS}" in prompt_to_send:
                                    prompt_to_send = prompt_to_send.replace(
                                        "{TARGET_LENGTH_CHARS}", str(st.session_state.original_length_chars))
                                prompt_to_send = re.sub(
                                    r'TARGET_LENGTH_CHARS:\s*\{[^}]*\}',
                                    f'TARGET_LENGTH_CHARS: {
    st.session_state.original_length_chars}',
                                    prompt_to_send
                                )
                                prompt_to_send = re.sub(
    r'SLIDE_COUNT:\s*\{[^}]*\}', '', prompt_to_send)
                                prompt_to_send = prompt_to_send.replace(
                                    "{SLIDE_COUNT}", "")

                            story_output = generate_text(prompt_to_send)
                            st.session_state.story_result_pending = story_output
                            st.session_state.generated_story = story_output
                            st.session_state.story_result = story_output
                            st.session_state.story_result_pending = None
                            st.session_state.story_prompt_filled = prompt_to_send

                            status.update(
    label="–ö—Ä–æ–∫ 3/3: –Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ ‚úÖ", state="complete")

                            update_status(
    "–ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
    "success",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—Å–µ",
     "–Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞")

                            try:
                                st.toast(
    "–ì–æ—Ç–æ–≤–æ: –Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞!", icon="‚úÖ")
                            except:
                                pass

                            st.rerun()
                        except Exception as e:
                            status.update(
    label=f"–ü–æ–º–∏–ª–∫–∞: {
        str(e)}", state="error")
                            update_status(
    f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–∞–π–ø–ª–∞–π–Ω—É: {
        str(e)}", "error", "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—Å–µ", "")
                            st.error(f"–ü–æ–º–∏–ª–∫–∞: {str(e)}")
                except:
                    # Fallback to spinner if st.status not available
                    with st.spinner("–ó–∞–ø—É—Å–∫ –ø–æ–≤–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω—É..."):
                        try:
                            # Same logic as above but with spinner
                            use_cache = True
                            raw_vtt, clean_text, meta = fetch_and_clean_subtitles(
                                st.session_state.youtube_url,
                                lang_mode=st.session_state.sub_lang_mode,
                                prefer_manual=st.session_state.prefer_manual,
                                use_cache=use_cache
                            )
                            st.session_state.raw_subtitles = raw_vtt
                            st.session_state.clean_subtitles = clean_text
                            st.session_state.subtitles_text = clean_text
                            st.session_state.original_length_chars = len(
                                clean_text)

                            if st.session_state.story_core_prompt_text:
                                st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                    st.session_state.story_core_prompt_text,
                                    clean_text
                                )
                            else:
                                try:
                                    template = load_prompt_file(
                                        str(STORY_CORE_PROMPT_PATH))
                                    st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                        template,
                                        clean_text
                                    )
                                except FileNotFoundError:
                                    pass

                            if not st.session_state.story_core_prompt_text or not st.session_state.story_core_prompt_text.strip():
                                raise ValueError("–ü—Ä–æ–º–ø—Ç STORY_CORE –ø–æ—Ä–æ–∂–Ω—ñ–π")

                            prompt_to_send = st.session_state.story_core_prompt_text
                            story_core_output = generate_text(
    prompt_to_send, backend=st.session_state.llm_backend)

                            st.session_state.story_core_text = story_core_output
                            st.session_state.story_core_result = story_core_output
                            import hashlib
                            st.session_state.story_core_result_hash = hashlib.md5(
                                story_core_output.encode()).hexdigest()
                            st.session_state.story_core_prompt_filled = prompt_to_send

                            story_core_str = st.session_state.story_core_result if st.session_state.story_core_result else ""
                            target_length_str = str(
    st.session_state.original_length_chars)
                            combined_vars = f"{story_core_str}|{target_length_str}"
                            current_hash = hashlib.md5(
                                combined_vars.encode()).hexdigest()

                            if (st.session_state.story_variables_hash is None or
                                st.session_state.story_variables_hash != current_hash or
                                not st.session_state.story_prompt_text or
                                st.session_state.story_prompt_text == st.session_state.story_prompt_template):

                                if not st.session_state.story_prompt_text or st.session_state.story_prompt_text == st.session_state.story_prompt_template:
                                    try:
                                        template = load_prompt_file(
                                            str(STORY_PROMPT_PATH))
                                        st.session_state.story_prompt_text = template
                                        st.session_state.story_prompt_template = template
                                    except FileNotFoundError:
                                        pass

                                if st.session_state.story_prompt_text:
                                    st.session_state.story_prompt_text = inject_all_story_variables(
                                        st.session_state.story_prompt_text,
                                        st.session_state.story_core_result if st.session_state.story_core_result else "",
                                        st.session_state.original_length_chars
                                    )
                                    st.session_state.story_variables_hash = current_hash

                            prompt_template = st.session_state.story_prompt_text
                            try:
                                prompt_to_send = fill_story_prompt(
                                    prompt_template,
                                    st.session_state.story_core_result,
                                    st.session_state.original_length_chars,
                                    None
                                )
                            except Exception:
                                prompt_to_send = prompt_template
                                if "{TARGET_LENGTH_CHARS}" in prompt_to_send:
                                    prompt_to_send = prompt_to_send.replace(
                                        "{TARGET_LENGTH_CHARS}", str(st.session_state.original_length_chars))
                                prompt_to_send = re.sub(
                                    r'TARGET_LENGTH_CHARS:\s*\{[^}]*\}',
                                    f'TARGET_LENGTH_CHARS: {
    st.session_state.original_length_chars}',
                                    prompt_to_send
                                )
                                prompt_to_send = re.sub(
    r'SLIDE_COUNT:\s*\{[^}]*\}', '', prompt_to_send)
                                prompt_to_send = prompt_to_send.replace(
                                    "{SLIDE_COUNT}", "")

                            story_output = generate_text(prompt_to_send)
                            st.session_state.story_result = story_output
                            st.session_state.generated_story = story_output
                            st.session_state.story_prompt_filled = prompt_to_send

                            update_status(
    "–ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
    "success",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—Å–µ",
     "–Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞")

                            try:
                                st.toast(
    "–ì–æ—Ç–æ–≤–æ: –Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞!", icon="‚úÖ")
                            except:
                                pass

                            st.rerun()
                        except Exception as e:
                            update_status(
    f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–∞–π–ø–ª–∞–π–Ω—É: {
        str(e)}", "error", "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—Å–µ", "")
                            st.error(f"–ü–æ–º–∏–ª–∫–∞: {str(e)}")

        # Handle STORY_CORE generation from action bar
        if generate_core_button:
            if not st.session_state.story_core_prompt_text or not st.session_state.story_core_prompt_text.strip():
                update_status(
    "–ü–æ–º–∏–ª–∫–∞: –ø—Ä–æ–º–ø—Ç STORY_CORE –ø–æ—Ä–æ–∂–Ω—ñ–π",
    "error",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE",
     "")
                st.error(
                    "–ü—Ä–æ–º–ø—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π. –ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —à–∞–±–ª–æ–Ω –∞–±–æ –≤–≤–µ–¥—ñ—Ç—å –ø—Ä–æ–º–ø—Ç.")
            else:
                try:
                    try:
                        with st.status("–ì–µ–Ω–µ—Ä—É—é STORY_CORE...", expanded=True) as status:
                            status.write("–ö—Ä–æ–∫ 1/1: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è STORY_CORE...")
                            prompt_to_send = st.session_state.story_core_prompt_text
                            from services.llm_client import generate_text
                            story_core_output = generate_text(prompt_to_send)
                            st.session_state.story_core_text_pending = story_core_output
                            st.session_state.story_core_result = story_core_output
                            import hashlib
                            st.session_state.story_core_result_hash = hashlib.md5(
                                story_core_output.encode()).hexdigest()
                            st.session_state.story_core_prompt_filled = prompt_to_send
                            st.session_state.story_core_text = story_core_output
                            st.session_state.story_core_text_pending = None
                            status.update(
    label="STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ ‚úÖ", state="complete")
                            update_status(
    "STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
    "success",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE",
     "STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ")
                            try:
                                st.toast(
    "–ì–æ—Ç–æ–≤–æ: STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ!", icon="‚úÖ")
                            except:
                                pass
                            st.rerun()
                    except:
                        with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è STORY_CORE..."):
                            prompt_to_send = st.session_state.story_core_prompt_text
                            from services.llm_client import generate_text
                            story_core_output = generate_text(prompt_to_send)
                            st.session_state.story_core_text_pending = story_core_output
                            st.session_state.story_core_result = story_core_output
                            import hashlib
                            st.session_state.story_core_result_hash = hashlib.md5(
                                story_core_output.encode()).hexdigest()
                            st.session_state.story_core_prompt_filled = prompt_to_send
                            st.session_state.story_core_text = story_core_output
                            st.session_state.story_core_text_pending = None
                            update_status(
    "STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
    "success",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE",
     "STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ")
                            try:
                                st.toast(
    "–ì–æ—Ç–æ–≤–æ: STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ!", icon="‚úÖ")
                            except:
                                pass
                            st.rerun()
                except Exception as e:
                    update_status(
    f"–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó STORY_CORE: {
        str(e)}", "error", "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE", "")
                    show_friendly_error(e)

        # Handle story generation from action bar
        if generate_story_button:
            # Check pipeline mode
            if st.session_state.pipeline_mode == "multipass":
                # Multi-PASS AI-controlled pipeline
                if not st.session_state.clean_subtitles or not st.session_state.clean_subtitles.strip():
                    update_status(
    "–ü–æ–º–∏–ª–∫–∞: –Ω–µ–º–∞—î —Å—É–±—Ç–∏—Ç—Ä—ñ–≤ –¥–ª—è Multi-PASS",
    "error",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (Multi-PASS)",
     "")
                    st.error("–ë—É–¥—å –ª–∞—Å–∫–∞, —Å–ø–æ—á–∞—Ç–∫—É –æ—Ç—Ä–∏–º–∞–π—Ç–µ —Å—É–±—Ç–∏—Ç—Ä–∏.")
                else:
                    try:
                        try:
                            with st.status("–ó–∞–ø—É—Å–∫ Multi-PASS pipeline...", expanded=True) as status:
                                from services.multipass_pipeline import run_multipass

                                status.write("PASS 0: –ê–Ω–∞–ª—ñ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏...")
                                multipass_result = run_multipass(
                                    st.session_state.clean_subtitles,
                                    target_chars=st.session_state.original_length_chars,
                                    slides_hint=None,
                                    backend=st.session_state.llm_backend
                                )

                                status.update(
    label="Multi-PASS –∑–∞–≤–µ—Ä—à–µ–Ω–æ ‚úÖ", state="complete")

                                # Format slides for display
                                slides = multipass_result.get(
                                    "story_slides", [])
                                slide_parts = []
                                for i, slide in enumerate(slides, 1):
                                    text = slide.get("Text", "")
                                    prompt = slide.get("Prompt", "")
                                    slide_parts.append(
                                        format_slide(text, prompt))
                                formatted_story = "\n".join(slide_parts)

                                # Store results
                                st.session_state.generated_story = formatted_story.strip()
                                st.session_state.story_result = formatted_story.strip()

                                # Store multipass debug info
                                st.session_state.debug_last_response = json.dumps(
                                    multipass_result, indent=2, ensure_ascii=False)

                                update_status(
    "Multi-PASS –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
    "success",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (Multi-PASS)",
     "Multi-PASS complete")
                                try:
                                    st.toast(
    "–ì–æ—Ç–æ–≤–æ: Multi-PASS —ñ—Å—Ç–æ—Ä—ñ—è!", icon="‚úÖ")
                                except:
                                    pass
                                st.rerun()
                        except:
                            with st.spinner("–ó–∞–ø—É—Å–∫ Multi-PASS pipeline..."):
                                from services.multipass_pipeline import run_multipass

                                multipass_result = run_multipass(
                                    st.session_state.clean_subtitles,
                                    target_chars=st.session_state.original_length_chars,
                                    slides_hint=None,
                                    backend=st.session_state.llm_backend
                                )

                                # Format slides for display
                                slides = multipass_result.get(
                                    "story_slides", [])
                                slide_parts = []
                                for i, slide in enumerate(slides, 1):
                                    text = slide.get("Text", "")
                                    prompt = slide.get("Prompt", "")
                                    slide_parts.append(
                                        format_slide(text, prompt))
                                formatted_story = "\n".join(slide_parts)

                                st.session_state.generated_story = formatted_story.strip()
                                st.session_state.story_result = formatted_story.strip()
                                st.session_state.debug_last_response = json.dumps(
                                    multipass_result, indent=2, ensure_ascii=False)

                                update_status(
    "Multi-PASS –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
    "success",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (Multi-PASS)",
     "Multi-PASS complete")
                                try:
                                    st.toast(
    "–ì–æ—Ç–æ–≤–æ: Multi-PASS —ñ—Å—Ç–æ—Ä—ñ—è!", icon="‚úÖ")
                                except:
                                    pass
                                st.rerun()
                    except Exception as e:
                        update_status(
                            f"–ü–æ–º–∏–ª–∫–∞ Multi-PASS: {str(e)}", "error", "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (Multi-PASS)", "")
                        show_friendly_error(e)
            elif not st.session_state.story_prompt_text or not st.session_state.story_prompt_text.strip():
                update_status("–ü–æ–º–∏–ª–∫–∞: –ø—Ä–æ–º–ø—Ç —ñ—Å—Ç–æ—Ä—ñ—ó –ø–æ—Ä–æ–∂–Ω—ñ–π",
                              "error", "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é", "")
                st.error(
                    "–ü—Ä–æ–º–ø—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π. –ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —à–∞–±–ª–æ–Ω –∞–±–æ –≤–≤–µ–¥—ñ—Ç—å –ø—Ä–æ–º–ø—Ç.")
            elif not st.session_state.story_core_result or not st.session_state.story_core_result.strip():
                update_status(
    "–ü–æ–º–∏–ª–∫–∞: STORY_CORE –Ω–µ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ",
    "error",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é",
     "")
                st.error(
                    "STORY_CORE –Ω–µ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ. –ë—É–¥—å –ª–∞—Å–∫–∞, —Å–ø–æ—á–∞—Ç–∫—É –∑–≥–µ–Ω–µ—Ä—É–π—Ç–µ STORY_CORE.")
            else:
                try:
                    try:
                        with st.status("–ì–µ–Ω–µ—Ä—É—é —ñ—Å—Ç–æ—Ä—ñ—é...", expanded=True) as status:
                            status.write("–ö—Ä–æ–∫ 1/1: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —ñ—Å—Ç–æ—Ä—ñ—ó...")
                            prompt_template = st.session_state.story_prompt_text
                            try:
                                prompt_to_send = fill_story_prompt(
                                    prompt_template,
                                    st.session_state.story_core_result,
                                    st.session_state.original_length_chars,
                                    None
                                )
                            except Exception:
                                prompt_to_send = prompt_template
                                if "{TARGET_LENGTH_CHARS}" in prompt_to_send:
                                    prompt_to_send = prompt_to_send.replace(
                                        "{TARGET_LENGTH_CHARS}", str(st.session_state.original_length_chars))
                                prompt_to_send = re.sub(
                                    r'TARGET_LENGTH_CHARS:\s*\{[^}]*\}',
                                    f'TARGET_LENGTH_CHARS: {
    st.session_state.original_length_chars}',
                                    prompt_to_send
                                )
                                prompt_to_send = re.sub(
    r'SLIDE_COUNT:\s*\{[^}]*\}', '', prompt_to_send)
                                prompt_to_send = prompt_to_send.replace(
                                    "{SLIDE_COUNT}", "")

                            story_output = generate_text(
    prompt_to_send, backend=st.session_state.llm_backend)
                            st.session_state.story_result_pending = story_output
                            st.session_state.generated_story = story_output
                            st.session_state.story_result = story_output
                            st.session_state.story_result_pending = None
                            st.session_state.story_prompt_filled = prompt_to_send
                            status.update(
    label="–Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ ‚úÖ", state="complete")
                            update_status(
    "–Ü—Å—Ç–æ—Ä—ñ—é –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
    "success",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é",
     "–Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞")
                            try:
                                st.toast(
    "–ì–æ—Ç–æ–≤–æ: –Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞!", icon="‚úÖ")
                            except:
                                pass
                            st.rerun()
                    except:
                        with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —ñ—Å—Ç–æ—Ä—ñ—ó..."):
                            prompt_template = st.session_state.story_prompt_text
                            try:
                                prompt_to_send = fill_story_prompt(
                                    prompt_template,
                                    st.session_state.story_core_result,
                                    st.session_state.original_length_chars,
                                    None
                                )
                            except Exception:
                                prompt_to_send = prompt_template
                                if "{TARGET_LENGTH_CHARS}" in prompt_to_send:
                                    prompt_to_send = prompt_to_send.replace(
                                        "{TARGET_LENGTH_CHARS}", str(st.session_state.original_length_chars))
                                prompt_to_send = re.sub(
                                    r'TARGET_LENGTH_CHARS:\s*\{[^}]*\}',
                                    f'TARGET_LENGTH_CHARS: {
    st.session_state.original_length_chars}',
                                    prompt_to_send
                                )
                                prompt_to_send = re.sub(
    r'SLIDE_COUNT:\s*\{[^}]*\}', '', prompt_to_send)
                                prompt_to_send = prompt_to_send.replace(
                                    "{SLIDE_COUNT}", "")

                            story_output = generate_text(
    prompt_to_send, backend=st.session_state.llm_backend)
                            st.session_state.story_result_pending = story_output
                            st.session_state.generated_story = story_output
                            st.session_state.story_result = story_output
                            st.session_state.story_result_pending = None
                            st.session_state.story_prompt_filled = prompt_to_send
                            update_status(
    "–Ü—Å—Ç–æ—Ä—ñ—é –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
    "success",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é",
     "–Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞")
                            try:
                                st.toast(
    "–ì–æ—Ç–æ–≤–æ: –Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞!", icon="‚úÖ")
                            except:
                                pass
                            st.rerun()
                except Exception as e:
                    update_status(
    f"–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —ñ—Å—Ç–æ—Ä—ñ—ó: {
        str(e)}", "error", "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é", "")
                    show_friendly_error(e)

        # Handle individual action buttons from action bar
        if fetch_subtitles_button:
            if not st.session_state.youtube_url or not st.session_state.youtube_url.strip():
                update_status(
    "–ü–æ–º–∏–ª–∫–∞: –Ω–µ –≤–∫–∞–∑–∞–Ω–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ YouTube",
    "error",
    "–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏",
     "")
                st.error("–ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ YouTube.")
            else:
                try:
                    try:
                        with st.status("–û—Ç—Ä–∏–º—É—é —Å—É–±—Ç–∏—Ç—Ä–∏...", expanded=True) as status:
                            status.write("–ö—Ä–æ–∫ 1/1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—É–±—Ç–∏—Ç—Ä—ñ–≤...")
                            use_cache = True
                            raw_vtt, clean_text, meta = fetch_and_clean_subtitles(
                                st.session_state.youtube_url,
                                lang_mode=st.session_state.sub_lang_mode,
                                prefer_manual=st.session_state.prefer_manual,
                                use_cache=use_cache
                            )
                            st.session_state.raw_subtitles = raw_vtt
                            st.session_state.clean_subtitles = clean_text
                            st.session_state.subtitles_text = clean_text
                            st.session_state.original_length_chars = len(
                                clean_text)

                            if st.session_state.story_core_prompt_text:
                                st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                    st.session_state.story_core_prompt_text,
                                    clean_text
                                )
                            else:
                                try:
                                    template = load_prompt_file(
                                        str(STORY_CORE_PROMPT_PATH))
                                    st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                        template,
                                        clean_text
                                    )
                                except FileNotFoundError:
                                    pass

                            source_map = {
                                "cache": "–∫–µ—à",
                                "manual": "—Ä—É—á–Ω—ñ",
                                "auto": "–∞–≤—Ç–æ"
                            }
                            source_text = source_map.get(
                                meta["source"], meta["source"])
                            status.update(
    label="–°—É–±—Ç–∏—Ç—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ ‚úÖ", state="complete")
                            update_status(
    f"–°—É–±—Ç–∏—Ç—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ! –î–∂–µ—Ä–µ–ª–æ: {source_text}",
    "success",
    "–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏",
     "–°—É–±—Ç–∏—Ç—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
                            try:
                                st.toast(
    "–ì–æ—Ç–æ–≤–æ: –°—É–±—Ç–∏—Ç—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ!", icon="‚úÖ")
                            except:
                                pass
                            st.rerun()
                    except:
                        with st.spinner("–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—É–±—Ç–∏—Ç—Ä—ñ–≤..."):
                            use_cache = True
                            raw_vtt, clean_text, meta = fetch_and_clean_subtitles(
                                st.session_state.youtube_url,
                                lang_mode=st.session_state.sub_lang_mode,
                                prefer_manual=st.session_state.prefer_manual,
                                use_cache=use_cache
                            )
                            st.session_state.raw_subtitles = raw_vtt
                            st.session_state.clean_subtitles = clean_text
                            st.session_state.subtitles_text = clean_text
                            st.session_state.original_length_chars = len(
                                clean_text)

                            if st.session_state.story_core_prompt_text:
                                st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                    st.session_state.story_core_prompt_text,
                                    clean_text
                                )
                            else:
                                try:
                                    template = load_prompt_file(
                                        str(STORY_CORE_PROMPT_PATH))
                                    st.session_state.story_core_prompt_text = inject_subtitles_into_prompt(
                                        template,
                                        clean_text
                                    )
                                except FileNotFoundError:
                                    pass

                            source_map = {
                                "cache": "–∫–µ—à",
                                "manual": "—Ä—É—á–Ω—ñ",
                                "auto": "–∞–≤—Ç–æ"
                            }
                            source_text = source_map.get(
                                meta["source"], meta["source"])
                            update_status(
    f"–°—É–±—Ç–∏—Ç—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ! –î–∂–µ—Ä–µ–ª–æ: {source_text}",
    "success",
    "–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏",
     "–°—É–±—Ç–∏—Ç—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
                            try:
                                st.toast(
    "–ì–æ—Ç–æ–≤–æ: –°—É–±—Ç–∏—Ç—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ!", icon="‚úÖ")
                            except:
                                pass
                            st.rerun()
                except Exception as e:
                    update_status(
    f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—É–±—Ç–∏—Ç—Ä—ñ–≤: {
        str(e)}", "error", "–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏", "")
                    st.error(f"–ü–æ–º–∏–ª–∫–∞: {str(e)}")

        # Main content area with expanders (one-page layout)

        # Check for pending story_core updates before creating widgets
        if st.session_state.story_core_text_pending is not None:
            st.session_state.story_core_text = st.session_state.story_core_text_pending
            st.session_state.story_core_result = st.session_state.story_core_text_pending
            # Update hash to trigger re-injection in story prompt
            import hashlib
            st.session_state.story_core_result_hash = hashlib.md5(
    st.session_state.story_core_text_pending.encode()).hexdigest()
            st.session_state.story_core_text_pending = None

        # Check for pending story result updates before creating widgets
        if st.session_state.story_result_pending is not None:
            st.session_state.story_result = st.session_state.story_result_pending
            st.session_state.generated_story = st.session_state.story_result_pending
            st.session_state.story_result_pending = None

        # Auto-inject all story variables (STORY_CORE, TARGET_LENGTH_CHARS)
        # if they exist and haven't been injected yet (or if they changed)
        import hashlib

        # Create combined hash for all variables
        story_core_str = st.session_state.story_core_result if st.session_state.story_core_result else ""
        target_length_str = str(st.session_state.original_length_chars)
        combined_vars = f"{story_core_str}|{target_length_str}"
        current_hash = hashlib.md5(combined_vars.encode()).hexdigest()

        # Check if we need to inject (hash changed or prompt text is empty/template)
        # If hash is None, it means variables were just set and haven't been
        # injected yet
        if (st.session_state.story_variables_hash is None or
            st.session_state.story_variables_hash != current_hash or
            not st.session_state.story_prompt_text or
            st.session_state.story_prompt_text == st.session_state.story_prompt_template):

            # If prompt text is empty or same as template, load template first
            if not st.session_state.story_prompt_text or st.session_state.story_prompt_text == st.session_state.story_prompt_template:
                try:
                    template = load_prompt_file(str(STORY_PROMPT_PATH))
                    st.session_state.story_prompt_text = template
                    st.session_state.story_prompt_template = template
                except FileNotFoundError:
                    pass

            # Inject all variables (STORY_CORE, TARGET_LENGTH_CHARS)
            if st.session_state.story_prompt_text:
                st.session_state.story_prompt_text = inject_all_story_variables(
                    st.session_state.story_prompt_text,
                    st.session_state.story_core_result if st.session_state.story_core_result else "",
                    st.session_state.original_length_chars
                )
                # Update hash to prevent re-injection on next rerun
                st.session_state.story_variables_hash = current_hash

        # Expander 1: Clean Subtitles
        with st.expander("–°—É–±—Ç–∏—Ç—Ä–∏ (–æ—á–∏—â–µ–Ω—ñ)", expanded=False):
            st.subheader("–û—á–∏—â–µ–Ω—ñ —Å—É–±—Ç–∏—Ç—Ä–∏")

            st.text_area(
                "–°—É–±—Ç–∏—Ç—Ä–∏ (–æ—á–∏—â–µ–Ω—ñ)",
                height=300,
                help="–û—á–∏—â–µ–Ω–∏–π —Ç–µ–∫—Å—Ç —Å—É–±—Ç–∏—Ç—Ä—ñ–≤ (ORIGINAL_STORY)",
                key="clean_subtitles"
            )

            # Recalculate length when text changes (text_area with key
            # automatically updates session_state)
            if 'clean_subtitles' in st.session_state:
                st.session_state.original_length_chars = len(
                    st.session_state.clean_subtitles)

            st.metric(
                "–ö-—Å—Ç—å —Å–∏–º–≤–æ–ª—ñ–≤ (–æ—Ä–∏–≥—ñ–Ω–∞–ª)",
                st.session_state.original_length_chars
            )

        # Expander 2: STORY_CORE Prompt (editable)
        with st.expander("–ü—Ä–æ–º–ø—Ç STORY_CORE (–º–æ–∂–Ω–∞ —Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏)", expanded=False):
            st.subheader("–ü—Ä–æ–º–ø—Ç STORY_CORE")

            st.text_area(
            "–®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç—É (–º–æ–∂–Ω–∞ —Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏)",
            height=300,
            help="–ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó STORY_CORE (—Å—É–±—Ç–∏—Ç—Ä–∏ –≤–∂–µ –ø—ñ–¥—Å—Ç–∞–≤–ª–µ–Ω—ñ)",
            key="story_core_prompt_text"
        )

        # Expander 3: STORY_CORE Result
        with st.expander("STORY_CORE —Ä–µ–∑—É–ª—å—Ç–∞—Ç", expanded=False):
            st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç STORY_CORE")
            st.text_area(
            "–ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π STORY_CORE",
            height=300,
            help="–†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó STORY_CORE (–º–æ–∂–Ω–∞ —Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏)",
            key="story_core_text"
        )

            col1, col2 = st.columns(2)
        with col1:
            if st.button("–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE", type="primary"):
                if not st.session_state.story_core_prompt_text or not st.session_state.story_core_prompt_text.strip():
                    update_status(
    "–ü–æ–º–∏–ª–∫–∞: –ø—Ä–æ–º–ø—Ç STORY_CORE –ø–æ—Ä–æ–∂–Ω—ñ–π",
    "error",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE (expander)",
     "")
                    st.error(
                        "–ü—Ä–æ–º–ø—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π. –ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —à–∞–±–ª–æ–Ω –∞–±–æ –≤–≤–µ–¥—ñ—Ç—å –ø—Ä–æ–º–ø—Ç.")
                else:
                    try:
                        try:
                            with st.status("–ì–µ–Ω–µ—Ä—É—é STORY_CORE...", expanded=True) as status:
                                status.write(
                                    "–ö—Ä–æ–∫ 1/1: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è STORY_CORE...")
                                prompt_to_send = st.session_state.story_core_prompt_text
                                from services.llm_client import generate_text
                                story_core_output = generate_text(
                                    prompt_to_send)
                                st.session_state.story_core_text_pending = story_core_output
                                st.session_state.story_core_result = story_core_output
                                import hashlib
                                st.session_state.story_core_result_hash = hashlib.md5(
                                    story_core_output.encode()).hexdigest()
                                st.session_state.story_core_prompt_filled = prompt_to_send
                                status.update(
    label="STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ ‚úÖ", state="complete")
                                update_status(
    "STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
    "success",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE (expander)",
     "STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ")
                                try:
                                    st.toast(
    "–ì–æ—Ç–æ–≤–æ: STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ!", icon="‚úÖ")
                                except:
                                    pass
                                st.rerun()
                        except:
                            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è STORY_CORE..."):
                                prompt_to_send = st.session_state.story_core_prompt_text
                                story_core_output = generate_text(
    prompt_to_send, backend=st.session_state.llm_backend)
                                st.session_state.story_core_text_pending = story_core_output
                                st.session_state.story_core_result = story_core_output
                                import hashlib
                                st.session_state.story_core_result_hash = hashlib.md5(
                                    story_core_output.encode()).hexdigest()
                                st.session_state.story_core_prompt_filled = prompt_to_send
                                update_status(
    "STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
    "success",
    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE (expander)",
     "STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ")
                                try:
                                    st.toast(
    "–ì–æ—Ç–æ–≤–æ: STORY_CORE –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ!", icon="‚úÖ")
                                except:
                                    pass
                                st.rerun()
                    except ValueError as e:
                        update_status(
    f"–ü–æ–º–∏–ª–∫–∞: {
        str(e)}",
        "error",
        "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE (expander)",
         "")
                        st.error(f"–ü–æ–º–∏–ª–∫–∞: {str(e)}")
                    except Exception as e:
                        update_status(
    f"–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {
        str(e)}",
        "error",
        "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE (expander)",
         "")
                        st.error(f"–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {str(e)}")

        with col2:
            if st.button("–ü–µ—Ä–µ–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ STORY_CORE (–∑ –ø–æ—Ç–æ—á–Ω–∏–º –ø—Ä–æ–º–ø—Ç–æ–º)"):
                if not st.session_state.story_core_prompt_text or not st.session_state.story_core_prompt_text.strip():
                    st.error(
                        "–ü—Ä–æ–º–ø—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π. –ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —à–∞–±–ª–æ–Ω –∞–±–æ –≤–≤–µ–¥—ñ—Ç—å –ø—Ä–æ–º–ø—Ç.")
                else:
                    try:
                        with st.spinner("–ü–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è STORY_CORE..."):
                            # Use EXACTLY the content of the top textarea
                            prompt_to_send = st.session_state.story_core_prompt_text

                            # Call LLM with the prompt
                            from services.llm_client import generate_text
                            story_core_output = generate_text(prompt_to_send)

                            # Store in pending (will be applied on next rerun
                            # before widget creation)
                            st.session_state.story_core_text_pending = story_core_output
                            st.session_state.story_core_result = story_core_output
                            # Update hash to trigger re-injection in story
                            # prompt
                            import hashlib
                            st.session_state.story_core_result_hash = hashlib.md5(
                                story_core_output.encode()).hexdigest()
                            st.session_state.story_core_prompt_filled = prompt_to_send
                            st.success("STORY_CORE –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
                            st.rerun()
                    except ValueError as e:
                        st.error(f"–ü–æ–º–∏–ª–∫–∞: {str(e)}")
                    except Exception as e:
                        st.error(f"–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {str(e)}")

        # Expander 4: Story Prompt (editable)
        with st.expander("–ü—Ä–æ–º–ø—Ç —ñ—Å—Ç–æ—Ä—ñ—ó (–º–æ–∂–Ω–∞ —Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏)", expanded=False):
            st.subheader("–ü—Ä–æ–º–ø—Ç —ñ—Å—Ç–æ—Ä—ñ—ó")
            st.text_area(
                "–®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç—É (–º–æ–∂–Ω–∞ —Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏)",
                height=300,
                help="–ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —ñ—Å—Ç–æ—Ä—ñ—ó (STORY_CORE –≤–∂–µ –ø—ñ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π)",
                key="story_prompt_text",
            )

        # Debug panel
        with st.expander("Debug / Raw AI response", expanded=False):
            from core.config import LLM_MODEL

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Model", LLM_MODEL)
                st.metric("TARGET_LENGTH_CHARS", st.session_state.debug_target_length_chars)
            with col2:
                st.metric("Prompt chars", st.session_state.debug_prompt_chars)
                st.metric("Story core chars", st.session_state.debug_story_core_chars)
            with col3:
                st.metric("Response chars", st.session_state.debug_response_chars)
                if st.session_state.debug_error:
                    st.error(f"Error: {st.session_state.debug_error}")

            st.divider()

            st.subheader("Prompt Preview (first 800 chars)")
            debug_last_prompt = st.session_state.get("debug_last_prompt", "")
            st.text_area(
                "Last prompt",
                value=debug_last_prompt[:800] if debug_last_prompt else "",
                height=150,
                disabled=True,
                key="debug_prompt_preview",
            )

            st.subheader("Response Preview (first 2000 chars)")
            debug_last_response = st.session_state.get("debug_last_response", "")
            st.text_area(
                "Last response",
                value=debug_last_response[:2000] if debug_last_response else "",
                height=200,
                disabled=True,
                key="debug_response_preview",
            )

            with st.expander("Full Prompt", expanded=False):
                st.text_area(
                    "Full prompt text",
                    value=debug_last_prompt,
                    height=300,
                    disabled=True,
                    key="debug_prompt_full",
                )

            with st.expander("Full Response", expanded=False):
                st.text_area(
                    "Full response text",
                    value=debug_last_response,
                    height=300,
                    disabled=True,
                    key="debug_response_full",
                )

        col1, col2 = st.columns(2)

        def _fill_story_prompt_for_send(prompt_template: str) -> str:
            """Fill story prompt placeholders; fallback to manual replacement."""
            try:
                return fill_story_prompt(
                    prompt_template,
                    st.session_state.story_core_result,
                    st.session_state.original_length_chars,
                    None,  # No slide_count
                )
            except Exception:
                prompt_to_send = prompt_template
                if "{TARGET_LENGTH_CHARS}" in prompt_to_send:
                    prompt_to_send = prompt_to_send.replace(
                        "{TARGET_LENGTH_CHARS}", str(st.session_state.original_length_chars)
                    )
                prompt_to_send = re.sub(
                    r"TARGET_LENGTH_CHARS:\s*\{[^}]*\}",
                    f"TARGET_LENGTH_CHARS: {st.session_state.original_length_chars}",
                    prompt_to_send,
                )
                prompt_to_send = re.sub(r"SLIDE_COUNT:\s*\{[^}]*\}", "", prompt_to_send)
                prompt_to_send = prompt_to_send.replace("{SLIDE_COUNT}", "")
                return prompt_to_send

        def _warn_on_placeholders(prompt_to_send: str) -> None:
            remaining_placeholders: list[str] = []
            if "{TARGET_LENGTH_CHARS}" in prompt_to_send or re.search(
                r"TARGET_LENGTH_CHARS:\s*\{", prompt_to_send
            ):
                remaining_placeholders.append("TARGET_LENGTH_CHARS")
            if "{{STORY_CORE}}" in prompt_to_send or "{STORY_CORE}" in prompt_to_send:
                remaining_placeholders.append("STORY_CORE")
            if remaining_placeholders:
                st.warning(
                    "–£–≤–∞–≥–∞: –ü–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∏ –Ω–µ –∑–∞–º—ñ–Ω–µ–Ω—ñ: "
                    f"{', '.join(remaining_placeholders)}. –ü—Ä–æ–¥–æ–≤–∂—É—é –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é..."
                )

        def _is_llm_refusal(text: str | None) -> bool:
            if not text:
                return False
            low = text.lower()
            return ("i'm sorry" in text) or ("i can't assist" in low) or (
                "cannot" in low and "assist" in low
            )

        with col1:
            if st.button("–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é", type="primary"):
                if not st.session_state.story_prompt_text or not st.session_state.story_prompt_text.strip():
                    update_status(
                        "–ü–æ–º–∏–ª–∫–∞: –ø—Ä–æ–º–ø—Ç —ñ—Å—Ç–æ—Ä—ñ—ó –ø–æ—Ä–æ–∂–Ω—ñ–π",
                        "error",
                        "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (expander)",
                        "",
                    )
                    st.error("–ü—Ä–æ–º–ø—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π. –ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —à–∞–±–ª–æ–Ω –∞–±–æ –≤–≤–µ–¥—ñ—Ç—å –ø—Ä–æ–º–ø—Ç.")
                elif not st.session_state.story_core_result or not st.session_state.story_core_result.strip():
                    update_status(
                        "–ü–æ–º–∏–ª–∫–∞: STORY_CORE –Ω–µ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ",
                        "error",
                        "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (expander)",
                        "",
                    )
                    st.error("STORY_CORE –Ω–µ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ. –ë—É–¥—å –ª–∞—Å–∫–∞, —Å–ø–æ—á–∞—Ç–∫—É –∑–≥–µ–Ω–µ—Ä—É–π—Ç–µ STORY_CORE.")
                else:
                    prompt_template = st.session_state.story_prompt_text
                    try:
                        with st.status("–ì–µ–Ω–µ—Ä—É—é —ñ—Å—Ç–æ—Ä—ñ—é...", expanded=True) as status:
                            status.write("–ö—Ä–æ–∫ 1/1: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —ñ—Å—Ç–æ—Ä—ñ—ó...")

                            prompt_to_send = _fill_story_prompt_for_send(prompt_template)
                            _warn_on_placeholders(prompt_to_send)

                            # Store debug info before calling LLM
                            st.session_state.debug_last_prompt = prompt_to_send
                            st.session_state.debug_prompt_chars = len(prompt_to_send)
                            st.session_state.debug_story_core_chars = (
                                len(st.session_state.story_core_result)
                                if st.session_state.story_core_result
                                else 0
                            )
                            st.session_state.debug_target_length_chars = (
                                st.session_state.original_length_chars
                            )

                            from services.llm_client import generate_text
                            try:
                                story_output = generate_text(prompt_to_send)
                                st.session_state.debug_error = None
                            except Exception as e:
                                st.session_state.debug_error = str(e)
                                raise

                            # Store debug info after LLM call
                            st.session_state.debug_last_response = story_output
                            st.session_state.debug_response_chars = (
                                len(story_output) if story_output else 0
                            )

                            if _is_llm_refusal(story_output):
                                status.update(label="–ü–æ–º–∏–ª–∫–∞: LLM –≤—ñ–¥–º–æ–≤–∏–≤—Å—è", state="error")
                                update_status(
                                    "LLM –≤—ñ–¥–º–æ–≤–∏–≤—Å—è –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç",
                                    "error",
                                    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (expander)",
                                    "",
                                )
                                st.error(
                                    "LLM –≤—ñ–¥–º–æ–≤–∏–≤—Å—è –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç. –ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:\n"
                                    "- –ü—Ä–æ–º–ø—Ç –º—ñ—Å—Ç–∏—Ç—å –Ω–µ–∑–∞–º—ñ–Ω–µ–Ω—ñ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∏\n"
                                    "- –ö–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Ä—É—à—É—î –ø–æ–ª—ñ—Ç–∏–∫—É OpenAI\n"
                                    "- –ü—Ä–æ–º–ø—Ç –∑–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∏–π –∞–±–æ –Ω–µ–∑—Ä–æ–∑—É–º—ñ–ª–∏–π\n\n"
                                    f"–í—ñ–¥–ø–æ–≤—ñ–¥—å LLM: {story_output[:200]}..."
                                )
                                st.session_state.story_result_pending = story_output
                                st.session_state.story_prompt_filled = prompt_to_send
                                st.rerun()
                            else:
                                st.session_state.story_result_pending = story_output
                                st.session_state.generated_story = story_output
                                st.session_state.story_result = story_output
                                st.session_state.story_result_pending = None
                                st.session_state.story_prompt_filled = prompt_to_send
                                status.update(label="–Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ ‚úÖ", state="complete")
                                update_status(
                                    "–Ü—Å—Ç–æ—Ä—ñ—é –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
                                    "success",
                                    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (expander)",
                                    "–Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞",
                                )
                                try:
                                    st.toast("–ì–æ—Ç–æ–≤–æ: –Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞!", icon="‚úÖ")
                                except Exception:
                                    pass
                                st.rerun()
                    except Exception as e:
                        # Fallback path if st.status isn't available or if it fails
                        with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —ñ—Å—Ç–æ—Ä—ñ—ó..."):
                            try:
                                prompt_to_send = _fill_story_prompt_for_send(prompt_template)
                                _warn_on_placeholders(prompt_to_send)

                                from services.llm_client import generate_text
                                story_output = generate_text(prompt_to_send)

                                st.session_state.story_result_pending = story_output
                                st.session_state.generated_story = story_output
                                st.session_state.story_result = story_output
                                st.session_state.story_result_pending = None
                                st.session_state.story_prompt_filled = prompt_to_send
                                update_status(
                                    "–Ü—Å—Ç–æ—Ä—ñ—é –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!",
                                    "success",
                                    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (expander)",
                                    "–Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞",
                                )
                                try:
                                    st.toast("–ì–æ—Ç–æ–≤–æ: –Ü—Å—Ç–æ—Ä—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞!", icon="‚úÖ")
                                except Exception:
                                    pass
                                st.rerun()
                            except ValueError as ve:
                                update_status(
                                    f"–ü–æ–º–∏–ª–∫–∞: {str(ve)}",
                                    "error",
                                    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (expander)",
                                    "",
                                )
                                st.error(f"–ü–æ–º–∏–ª–∫–∞: {str(ve)}")
                            except Exception as ge:
                                update_status(
                                    f"–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {str(ge)}",
                                    "error",
                                    "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (expander)",
                                    "",
                                )
                                st.error(f"–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {str(ge)}")
                                import traceback

                                st.code(traceback.format_exc())

        with col2:
            if st.button("–ü–µ—Ä–µ–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (–∑ –ø–æ—Ç–æ—á–Ω–∏–º –ø—Ä–æ–º–ø—Ç–æ–º)"):
                if not st.session_state.story_prompt_text or not st.session_state.story_prompt_text.strip():
                    st.error("–ü—Ä–æ–º–ø—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π. –ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —à–∞–±–ª–æ–Ω –∞–±–æ –≤–≤–µ–¥—ñ—Ç—å –ø—Ä–æ–º–ø—Ç.")
                else:
                    try:
                        with st.spinner("–ü–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è —ñ—Å—Ç–æ—Ä—ñ—ó..."):
                            prompt_template = st.session_state.story_prompt_text
                            prompt_to_send = _fill_story_prompt_for_send(prompt_template)
                            _warn_on_placeholders(prompt_to_send)

                            st.session_state.debug_last_prompt = prompt_to_send
                            st.session_state.debug_prompt_chars = len(prompt_to_send)
                            st.session_state.debug_story_core_chars = (
                                len(st.session_state.story_core_result)
                                if st.session_state.story_core_result
                                else 0
                            )
                            st.session_state.debug_target_length_chars = (
                                st.session_state.original_length_chars
                            )

                            from services.llm_client import generate_text
                            try:
                                story_output = generate_text(prompt_to_send)
                                st.session_state.debug_error = None
                            except Exception as e:
                                st.session_state.debug_error = str(e)
                                raise

                            st.session_state.debug_last_response = story_output
                            st.session_state.debug_response_chars = (
                                len(story_output) if story_output else 0
                            )

                            if _is_llm_refusal(story_output):
                                st.error(
                                    "LLM –≤—ñ–¥–º–æ–≤–∏–≤—Å—è –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç. –ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:\n"
                                    "- –ü—Ä–æ–º–ø—Ç –º—ñ—Å—Ç–∏—Ç—å –Ω–µ–∑–∞–º—ñ–Ω–µ–Ω—ñ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∏\n"
                                    "- –ö–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Ä—É—à—É—î –ø–æ–ª—ñ—Ç–∏–∫—É OpenAI\n"
                                    "- –ü—Ä–æ–º–ø—Ç –∑–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∏–π –∞–±–æ –Ω–µ–∑—Ä–æ–∑—É–º—ñ–ª–∏–π\n\n"
                                    f"–í—ñ–¥–ø–æ–≤—ñ–¥—å LLM: {story_output[:200]}..."
                                )
                                st.session_state.story_result_pending = story_output
                                st.session_state.story_prompt_filled = prompt_to_send
                                st.rerun()
                            else:
                                st.session_state.story_result_pending = story_output
                                st.session_state.story_prompt_filled = prompt_to_send
                                st.success("–Ü—Å—Ç–æ—Ä—ñ—é –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
                                st.rerun()
                    except ValueError as ve:
                        st.error(f"–ü–æ–º–∏–ª–∫–∞: {str(ve)}")
                    except Exception as ge:
                        st.error(f"–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {str(ge)}")
                        import traceback

                        st.code(traceback.format_exc())

        # Expander 5: Story Result (auto-expands when story exists)
        has_story = bool(
            st.session_state.get("generated_story") or st.session_state.get("story_result")
        )
        with st.expander("–†–µ–∑—É–ª—å—Ç–∞—Ç —ñ—Å—Ç–æ—Ä—ñ—ó", expanded=has_story):
            st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç —ñ—Å—Ç–æ—Ä—ñ—ó")
            st.text_area(
                "–ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ —ñ—Å—Ç–æ—Ä—ñ—è",
                height=300,
                help="–†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —ñ—Å—Ç–æ—Ä—ñ—ó (–º–æ–∂–Ω–∞ —Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏)",
                key="generated_story",
            )

            # Add code block for easy copying
            st.code(st.session_state.generated_story or "", language=None)
    
    # ========================
    # TAB 2: ANALYTICS
    # ========================
    with tab_analytics:
        # Analytics and Improvement
        st.subheader("–ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ —Ç–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è")
        
        # Apply pending updates BEFORE creating widgets
        if st.session_state.analysis_story_input_pending is not None:
            st.session_state.analysis_story_input = st.session_state.analysis_story_input_pending
            st.session_state.analysis_story_input_pending = None
        
        # Single source of truth: story input field
        st.text_area(
            "–Ü—Å—Ç–æ—Ä—ñ—è –¥–ª—è –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ (–≤—Å—Ç–∞–≤ —Å—é–¥–∏)",
            key="analysis_story_input",
            height=350,
            help="–í—Å—Ç–∞–≤ —ñ—Å—Ç–æ—Ä—ñ—é –≤ —Ü–µ –ø–æ–ª–µ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ç–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è"
        )
        
        # Convenience buttons
        col_a, col_b = st.columns(2)
        with col_a:
            if st.button("–í—Å—Ç–∞–≤–∏—Ç–∏ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—É —ñ—Å—Ç–æ—Ä—ñ—é —Å—é–¥–∏"):
                if st.session_state.get("generated_story") and st.session_state.generated_story.strip():
                    st.session_state.analysis_story_input_pending = st.session_state.generated_story
                    st.success("–Ü—Å—Ç–æ—Ä—ñ—é –≤—Å—Ç–∞–≤–ª–µ–Ω–æ!")
                    st.rerun()
                else:
                    st.warning("–ù–µ–º–∞—î –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ—ó —ñ—Å—Ç–æ—Ä—ñ—ó –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏.")
        
        with col_b:
            if st.button("–ó–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ –ø–æ–∫—Ä–∞—â–µ–Ω—É —è–∫ –≤—Ö—ñ–¥"):
                if st.session_state.get("improved_story") and st.session_state.improved_story.strip():
                    st.session_state.analysis_story_input_pending = st.session_state.improved_story
                    st.success("–ü–æ–∫—Ä–∞—â–µ–Ω—É —ñ—Å—Ç–æ—Ä—ñ—é –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω–æ —è–∫ –≤—Ö—ñ–¥!")
                    st.rerun()
                else:
                    st.warning("–ù–µ–º–∞—î –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó —ñ—Å—Ç–æ—Ä—ñ—ó –¥–ª—è –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è.")
        
        st.divider()
        
        # Validate if we have input
        has_story_input = bool(st.session_state.get("analysis_story_input") and st.session_state.analysis_story_input.strip())
        has_improvement_prompt = bool(st.session_state.get("improvement_prompt"))
        
        col1, col2 = st.columns(2)
        with col1:
            analyze_button = st.button(
                "–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è (EN)",
                type="primary",
                disabled=not has_story_input
            )
        with col2:
            improve_button = st.button(
                "–ü–æ–∫—Ä–∞—â–∏—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (EN)",
                type="primary",
                disabled=not has_story_input or not has_improvement_prompt
            )
        
        if not has_story_input:
            st.info("–í—Å—Ç–∞–≤ —ñ—Å—Ç–æ—Ä—ñ—é —É –ø–æ–ª–µ –≤–∏—â–µ, —â–æ–± –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ—Ç–∏–∫—É.")
        elif not has_improvement_prompt and improve_button:
            st.warning("–°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ —ñ—Å—Ç–æ—Ä—ñ—é, —â–æ–± —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è.")
        
        # Handle analyze button
        if analyze_button:
            # Validate input
            story = st.session_state.analysis_story_input.strip()
            if not story:
                st.error("–í—Å—Ç–∞–≤ —ñ—Å—Ç–æ—Ä—ñ—é –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —É –ø–æ–ª–µ –≤–∏—â–µ.")
            else:
                try:
                    with st.spinner("–ê–Ω–∞–ª—ñ–∑ —ñ—Å—Ç–æ—Ä—ñ—ó..."):
                        # Prepare original subtitles (truncate if too long, but keep key parts)
                        original_text = st.session_state.get("clean_subtitles", "")
                        if len(original_text) > 5000:
                            # Keep first 2000 and last 2000 chars to preserve key parts
                            original_text = original_text[:2000] + "\n\n[...—Ç–µ–∫—Å—Ç —Å–∫–æ—Ä–æ—á–µ–Ω–æ...]\n\n" + original_text[-2000:]
                        
                        # Use the dedicated input field as single source of truth
                        generated_text = story
                        
                        # Build analysis prompt with English-only instruction at the top
                        analysis_prompt = f"""OUTPUT LANGUAGE: For the "–ü–†–û–ú–ü–¢ –î–õ–Ø –ü–û–ö–†–ê–©–ï–ù–ù–Ø" section, output ENGLISH ONLY. Do not output any Ukrainian/Russian in that section.

{ANALYSIS_PROMPT_TEMPLATE.format(
                            ORIGINAL=original_text,
                            GENERATED=generated_text
                        )}"""
                        
                        # Call LLM
                        analysis_response = generate_text(analysis_prompt, backend=st.session_state.llm_backend)
                        
                        # Parse response
                        analysis_report, comparison_table_md, improvement_prompt = parse_analysis_response(analysis_response)
                        
                        # Check if improvement_prompt contains Cyrillic characters
                        cyrillic_pattern = re.compile(r'[–ê-–Ø–∞-—è–Ü—ñ–á—ó–Ñ—î“ê“ë]')
                        if improvement_prompt and cyrillic_pattern.search(improvement_prompt):
                            # Retry once with stronger English-only instruction
                            st.warning("–ü—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º—ñ—Å—Ç–∏—Ç—å –Ω–µ–∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ —Å–∏–º–≤–æ–ª–∏. –ü–æ–≤—Ç–æ—Ä–Ω–∞ —Å–ø—Ä–æ–±–∞...")
                            retry_prompt = f"""OUTPUT LANGUAGE: ENGLISH ONLY. Do not output any Ukrainian/Russian.

{ANALYSIS_PROMPT_TEMPLATE.format(
                                ORIGINAL=original_text,
                                GENERATED=generated_text
                            )}

CRITICAL: The "–ü–†–û–ú–ü–¢ –î–õ–Ø –ü–û–ö–†–ê–©–ï–ù–ù–Ø" section must be in ENGLISH ONLY. No other language."""
                            retry_response = generate_text(retry_prompt, backend=st.session_state.llm_backend)
                            analysis_report, comparison_table_md, improvement_prompt = parse_analysis_response(retry_response)
                            
                            # Check again
                            if improvement_prompt and cyrillic_pattern.search(improvement_prompt):
                                st.error("–ú–æ–¥–µ–ª—å –ø–æ–≤–µ—Ä–Ω—É–ª–∞ –ø—Ä–æ–º–ø—Ç –Ω–µ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –ø—ñ—Å–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ—ó —Å–ø—Ä–æ–±–∏. –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑.")
                                # Still store what we got, but show error
                        
                        # Store in session state
                        st.session_state.analysis_report = analysis_report
                        st.session_state.comparison_table_md = comparison_table_md
                        st.session_state.improvement_prompt = improvement_prompt
                        
                        st.success("–ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
                        st.rerun()
                except Exception as e:
                    st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –∞–Ω–∞–ª—ñ–∑—É: {str(e)}")
        
        # Handle improve button
        if improve_button:
            # Validate input
            story = st.session_state.analysis_story_input.strip()
            if not story:
                st.error("–í—Å—Ç–∞–≤ —ñ—Å—Ç–æ—Ä—ñ—é –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —É –ø–æ–ª–µ –≤–∏—â–µ.")
            elif not has_improvement_prompt:
                st.error("–°–ø–æ—á–∞—Ç–∫—É –∑–≥–µ–Ω–µ—Ä—É–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è.")
            else:
                try:
                    with st.spinner("–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó..."):
                        # Use the dedicated input field as single source of truth
                        generated_text = story
                        improvement_prompt_text = st.session_state.get("improvement_prompt", "")
                        
                        # Build improve prompt with English-only instruction and format requirements
                        improve_prompt = f"""OUTPUT LANGUAGE: ENGLISH ONLY. Do not output any Ukrainian/Russian.

{improvement_prompt_text}

ORIGINAL GENERATED STORY (to rewrite):
    {generated_text}

Rewrite the story according to the instructions above, preserving key facts and improving identified weaknesses.

STRICT FORMAT REQUIREMENTS:
    - Each slide must be exactly:
        text:{{...}}
  prompt:{{...}}
- No headings, no numbering, no markdown.
- Preserve the exact slide structure.
- Output in ENGLISH ONLY.

ANTI-COPY RULES (CRITICAL):
    - You MUST rewrite every text:{{...}} block.
- DO NOT reuse original sentences.
- Change wording in EVERY slide while keeping meaning.
- If output is too similar to input, rewrite more aggressively.
- Paraphrase, rephrase, restructure - but keep the core facts and narrative flow.
- This is a REWRITE task, not a copy-paste task."""
                        
                        # Call LLM
                        improved_story = generate_text(improve_prompt, backend=st.session_state.llm_backend)
                        
                        # Validate: Check for Cyrillic characters
                        cyrillic_pattern = re.compile(r'[–ê-–Ø–∞-—è–Ü—ñ–á—ó–Ñ—î“ê“ë]')
                        if cyrillic_pattern.search(improved_story):
                            st.error("–ú–æ–¥–µ–ª—å –ø–æ–≤–µ—Ä–Ω—É–ª–∞ –Ω–µ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é. –ù–∞—Ç–∏—Å–Ω–∏ '–ü–æ–∫—Ä–∞—â–∏—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é (EN)' —â–µ —Ä–∞–∑.")
                            # Do NOT store improved_story when invalid
                        else:
                            # Similarity guard: check if output is too similar to input
                            def normalize_text(text):
                                """Normalize text for similarity comparison."""
                                return ' '.join(text.lower().split())
                            
                            normalized_input = normalize_text(generated_text)
                            normalized_improved = normalize_text(improved_story)
                            
                            # Calculate similarity ratio
                            similarity_ratio = difflib.SequenceMatcher(None, normalized_input, normalized_improved).ratio()
                            
                            if similarity_ratio > 0.97:
                                st.warning(f"–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤–∏–π—à–ª–æ —ñ–¥–µ–Ω—Ç–∏—á–Ω–∏–º (—Å—Ö–æ–∂—ñ—Å—Ç—å: {similarity_ratio:.2%}). –ù–∞—Ç–∏—Å–Ω–∏ —â–µ —Ä–∞–∑ –∞–±–æ –ø—ñ–¥–∫—Ä—É—Ç–∏ –ø—Ä–æ–º–ø—Ç.")
                                # Do NOT update improved_story when too similar
                            else:
                                # Store in session state only if valid and different
                                st.session_state.improved_story = improved_story
                                st.success(f"–Ü—Å—Ç–æ—Ä—ñ—é –ø–æ–∫—Ä–∞—â–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ! –°—Ö–æ–∂—ñ—Å—Ç—å –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª–æ–º: {similarity_ratio:.2%}")
                                st.rerun()
                except Exception as e:
                    st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {str(e)}")
        
        # Display analysis report
        if st.session_state.get("analysis_report"):
            st.divider()
            st.subheader("–ó–≤—ñ—Ç –∞–Ω–∞–ª—ñ–∑—É")
            # Use markdown for better formatting, or text_area readonly
            st.markdown(st.session_state.analysis_report)
        
        # Display comparison table
        if st.session_state.get("comparison_table_md"):
            st.divider()
            st.subheader("–¢–∞–±–ª–∏—Ü—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è")
            st.markdown(st.session_state.comparison_table_md)
        
        # Display improvement prompt (editable)
        if st.session_state.get("improvement_prompt"):
            st.divider()
            st.subheader("–ü—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è")
        st.text_area(
                "–ü—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è (–º–æ–∂–Ω–∞ —Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏)",
                height=200,
                help="–ü—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó (–º–æ–∂–Ω–∞ —Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏ –ø–µ—Ä–µ–¥ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º)",
                key="improvement_prompt"
            )
            st.code(st.session_state.improvement_prompt or "", language=None)
        
        # Display improved story (editable) - separate output field
        if st.session_state.get("improved_story"):
            st.divider()
            st.subheader("–ü–æ–∫—Ä–∞—â–µ–Ω–∞ —ñ—Å—Ç–æ—Ä—ñ—è (—Ä–µ–∑—É–ª—å—Ç–∞—Ç)")
            st.text_area(
                "–ü–æ–∫—Ä–∞—â–µ–Ω–∞ —ñ—Å—Ç–æ—Ä—ñ—è (—Ä–µ–∑—É–ª—å—Ç–∞—Ç)",
            height=300,
                help="–ü–æ–∫—Ä–∞—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è —ñ—Å—Ç–æ—Ä—ñ—ó (–º–æ–∂–Ω–∞ —Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏)",
                key="improved_story"
        )
            st.code(st.session_state.improved_story or "", language=None)


if __name__ == "__main__":
    main()

